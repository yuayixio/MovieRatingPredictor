{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Content-based Filtering\n",
    "\n",
    "Aproach:\n",
    "- Table mit Features und Ratings erstellen, ohne User - auf Basis des OMDB Datasets\n",
    "\n",
    "- sklearn-kNN mit cosine-similarity darauf anwenden\n",
    "\n",
    "- Funktion schreiben, die auf basis des ratings der neighbours das rating eines Filmes vorhersagt.\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "#### https://heartbeat.fritz.ai/recommender-systems-with-python-part-i-content-based-filtering-5df4940bd831\n",
    "\n",
    "#### https://www.kaggle.com/johnwill225/movie-recommendations\n",
    "\n",
    "#### https://towardsdatascience.com/how-we-built-a-content-based-filtering-recommender-system-for-music-with-python-c6c3b1020332\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from knn_preprocessing import knn_preprocessing\n",
    "\n",
    "kwargs = dict(random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../../data/preprocessed/movies_id_updated.csv')\n",
    "omdb = pd.read_csv('../../data/preprocessed/omdb_cleaned.csv')\n",
    "ratings = pd.read_csv('../../data/preprocessed/ratings_clean_std_0.csv')\n",
    "genres = pd.read_csv('../../data/raw/genres.csv', sep=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movies = movies.drop(columns={'spanishTitle','imdbPictureURL','rtID','rtPictureURL'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movies['imdbID'] = movies['imdbID'].str.replace(r'tt', '')\n",
    "movies['imdbID'] = movies['imdbID'].astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mapping = movies[['id', 'imdbID']].rename(columns={'id':'movieID'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "omdb.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Moved Preprocessing to own script\n",
    "merged_data = knn_preprocessing(['imdbID', 'Year', 'Runtime', 'Language', 'imdbRating', 'imdbVotes', 'Rotten Tomatoes', 'Metacritic',\n",
    "       'Series', 'PG_Rating', 'Oscars_won', 'Oscars_nominated',\n",
    "       'Golden_globe_won', 'Golden_globe_nominated'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function that computes a rating based on the neighbors\n",
    "def compute_rating(neighbors, distances, mean = False):\n",
    "    \n",
    "    if mean == True:\n",
    "        pred = neighbors.mean()\n",
    "    else:\n",
    "        #scaling ratings based on distance\n",
    "        pred = sum(neighbors* (1+(1-distances[0]/distances[0].mean()))) / neighbors.shape[0]\n",
    "    \n",
    "    return float(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First Approach for easy k tuning - use method later to implement in depth tuning of k\n",
    "\n",
    "def adjust_k(ratings):\n",
    "    adjusted_k = 10\n",
    "    r_size = len(ratings)\n",
    "    \n",
    "    if r_size > 40 and r_size  < 100:\n",
    "        adjusted_k = 15\n",
    "    elif r_size  > 100 and r_size < 500:\n",
    "        adjusted_k = 20\n",
    "    elif r_size  > 500 and r_size < 1500:\n",
    "        adjusted_k = 25\n",
    "    elif r_size  > 1500:\n",
    "        adjusted_k = 30\n",
    "        #print(r_size) \n",
    "        \n",
    "    return adjusted_k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## function that predicts the rating of a movie from its imdbID and its nearest neighbors\n",
    "\n",
    "def predict_movie_rating(imdbID, userID,user_data=merged_data, mean=False, knn_metric='cosine', set_k=False, k_neighbors=15):\n",
    "     \n",
    "    # Select all ratings given by User #userID\n",
    "    ratings = user_data.loc[user_data['user_id'] == userID]\n",
    "    \n",
    "    #If no explicit number of neighbors is passed -> use variable neighbors function\n",
    "    if set_k:\n",
    "        k_neighbors = k_neighbors\n",
    "    else:    \n",
    "        k_neighbors = adjust_k(ratings)\n",
    "\n",
    "  \n",
    "    # Get real rating -> remove this in the end -> currently done for validation\n",
    "    real_ratings = ratings.loc[(ratings['imdbID'] == imdbID)]\n",
    "    \n",
    "    real_idx = ratings.loc[(ratings['imdbID'] == imdbID)].index\n",
    "    \n",
    "    #remove real rating\n",
    "    ratings = ratings[ratings['imdbID'] != imdbID] \n",
    "\n",
    "    #Scaling features -> maybe do outside function in future\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    features = pd.DataFrame(scaler.fit_transform(ratings.drop(columns = {'imdbID','user_id', 'rating'}))).merge(pd.DataFrame(ratings.index), left_index=True, right_index=True, how='left')\n",
    "      \n",
    "    \n",
    "    if (ratings.to_numpy().size>0):   \n",
    "        \n",
    "        # Set algorithm and params\n",
    "        if knn_metric == 'minkowski':\n",
    "            knn = NearestNeighbors(metric='minkowski',p=2 , algorithm='brute', n_neighbors=k_neighbors, n_jobs=-1)\n",
    "        else:    \n",
    "            knn = NearestNeighbors(metric=knn_metric , algorithm='brute', n_neighbors=k_neighbors, n_jobs=-1)\n",
    "\n",
    "        # Training\n",
    "        #print('---- Training ConBF-kNN-Algorithm ----')\n",
    "        #print('user_id: '+str(userID))\n",
    "        #print('imdbID: '+str(imdbID))\n",
    "        \n",
    "    \n",
    "        knn.fit(csr_matrix(features.iloc[:,0:(user_data.shape[1]-3)]))\n",
    "        \n",
    "        input_data = user_data.iloc[real_idx]\n",
    "        inputs = scaler.transform(input_data.drop(columns = {'imdbID','user_id', 'rating'}))\n",
    "        \n",
    "    \n",
    "        #Prediction -> get x nearest neighbors of imdbID\n",
    "        distances , indices = knn.kneighbors(inputs, n_neighbors=k_neighbors)\n",
    "        \n",
    "       # Zieht indices und ratings der neighbors\n",
    "        neighbor_ratings = user_data['rating'].loc[features['0_y'].loc[indices[0]]]\n",
    "      \n",
    "        # compute rating of movie(imbdID) based on the rating of the 20 nearest neighbors\n",
    "        #mean = True gibt nur mittelwert der nachbarn\n",
    "        \n",
    "        pred = compute_rating(neighbor_ratings, distances, mean)\n",
    "        \n",
    "        #Generate Output for Understandability\n",
    "        #print('Predicted Rating for '+str(imdbID)+': '+str(pred))\n",
    "        #print('Real Rating of '+str(imdbID)+' was: '+ str(real_ratings['rating'].values[0]))\n",
    "        \n",
    "        \n",
    "        #Output to understand mistakes\n",
    "        #neighbor_data = ratings.loc[features['0_y'].loc[indices[0]]]\n",
    "        \n",
    "        #neighbor_movies = neighbor_data.merge(movies, how='left', on='imdbID')\n",
    "        \n",
    "        #for i in range (0, len(neighbor_data)):\n",
    "        #    print(genres_grouped[genres_grouped['imdbID']==neighbor_data['imdbID'].values[i]])\n",
    "        \n",
    "        #print(neighbor_movies)\n",
    "        #print(neighbor_movies.describe())\n",
    "    \n",
    "    \n",
    "        # return rating prediction and real rating\n",
    "        return pred , real_ratings['rating'].values[0]\n",
    "        \n",
    "    else:\n",
    "         return \"User has not rated other movies. Check input\"\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Keeping this for future testing\n",
    "### Testing function for ToyStory###\n",
    "#imdbID = 114709.0\n",
    "# Aufpassen userID und imdbID als float Ã¼bergeben!! User: 394,1171, 3682\n",
    "#userID = 394\n",
    "#pred , real = predict_movie_rating(imdbID, userID, merged_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Larger Test:\n",
    "\n",
    "def test_predict_mr(no_test_samples, mean = False, knn_metric = 'cosine', set_k=False, k_neighbors=15):\n",
    "    # Sampling #no_test_samples of random samples from dataset for testing\n",
    "    test_set = merged_data.sample(n=no_test_samples)\n",
    "    \n",
    "    predictions = pd.DataFrame(columns=['Prediction'])\n",
    "    reals = pd.DataFrame(columns=['Real_Rating'])\n",
    "    \n",
    "    # Iterate over test-set and generate predicitons for it\n",
    "    # TODO get rid of ugly for-loop\n",
    "    for row in test_set.itertuples():\n",
    "        imdbID = row.imdbID\n",
    "        userID = row.user_id\n",
    "        pred , real = predict_movie_rating(imdbID, userID, merged_data, mean, knn_metric, set_k=False, k_neighbors=15)\n",
    "        predictions.loc[row[0]] = pred\n",
    "        reals.loc[row[0]] = real\n",
    "    \n",
    "    rmse = mean_squared_error(reals['Real_Rating'], predictions['Prediction'], squared=False)\n",
    "    print('RMSE: '+str(rmse))\n",
    "    return float(rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test_predict_mr(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"PCA:\")\n",
    "merged_g=merged_data.drop(columns={'user_id','imdbID','rating'})\n",
    "scaler = preprocessing.StandardScaler()\n",
    "merged_gt = scaler.fit_transform(merged_g)\n",
    "pca = PCA().fit(merged_gt)\n",
    "top_PCA=[\"%.2f\" % a for a in pca.explained_variance_ratio_ if a >0.01]\n",
    "print(\"Main Variance impacting factors:\")\n",
    "print(pca.explained_variance_ratio_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "first_comp = pca.components_[0]\n",
    "first_comps = pd.DataFrame(zip(first_comp, merged_g.columns), columns=['weights', 'features'])\n",
    "first_comps['abs_weights']=first_comps['weights'].apply(lambda x: np.abs(x))\n",
    "first_comps= first_comps.sort_values('abs_weights', ascending=False)\n",
    "print(first_comps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda-analytics-challenge-ss2020",
   "language": "python",
   "name": "bda-analytics-challenge-ss2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}